apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: runai
  name: runai
  namespace: monitoring
spec:
  groups:
  - interval: 10s
    name: runai-rules
    rules:
    - expr: (sum(runai_pod_group_info) by (pod_group_uuid, detailed_status, node_name,
        node, queue_name, user, workload_name, workload_type, job_name, project, job_type,
        job_uuid)) * on (pod_group_uuid) group_right(detailed_status, node_name, node,
        queue_name, user, workload_name, workload_type, job_name, project, job_type,
        job_uuid) (sum(runai_pod_group_phase) by (pod_group_uuid, job_uuid, phase,
        status))
      record: runai_pod_group_phase_with_info
    - expr: runai_pod_info * on (pod_uuid) group_right(queue_name, pod_group_name,
        workload_name, workload_type, node_name, node, gpu_index, gpu, job_name, job_uuid,
        job_type, node, project) (sum(runai_pod_phase) by (pod_uuid, phase, status,
        pod_namespace, pod_name, pod_group_uuid, job_uuid))
      record: runai_pod_phase_with_info
    - expr: label_replace(label_replace(DCGM_FI_DEV_GPU_UTIL, "pod_name", "$1", "exported_pod",
        "(.+)"), "pod_namespace", "$1", "exported_namespace", "(.+)")
      record: dcgm_gpu_utilization
    - expr: count by (instance, gpu) (dcgm_gpu_utilization{pod_name=~".+"} and on(pod_name,
        pod_namespace) (runai_pod_phase{phase="Running"} == 1))
      record: runai_gpus_running_with_pod
    - expr: runai_gpus_running_with_pod or ((count by (instance,gpu) (dcgm_gpu_utilization)
        unless runai_gpus_running_with_pod) - 1)
      record: runai_gpus_is_running_with_pod
    - expr: sum without (pod_ip, instance)((label_replace(runai_gpus_is_running_with_pod,
        "pod_ip", "$1", "instance", "(.*):(.*)")) * on(pod_ip) group_left(node) kube_pod_info{created_by_name=~".*dcgm-exporter"})
      record: runai_gpus_is_running_with_pod2
    - expr: sum by ( gpu, node, pod_name, pod_namespace) ((label_replace(dcgm_gpu_utilization,
        "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~".*dcgm-exporter"})
      record: runai_node_gpu_utilization
    - expr: (label_replace(DCGM_FI_DEV_FB_FREE, "pod_ip", "$1", "instance", "(.*):(.*)"))
        * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~".*dcgm-exporter"}
      record: runai_node_gpu_free_memory_excluding_mig
    - expr: (label_replace(DCGM_FI_DEV_FB_USED, "pod_ip", "$1", "instance", "(.*):(.*)"))
        * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~".*dcgm-exporter"}
      record: runai_node_gpu_used_memory_excluding_mig
    - expr: runai_node_gpu_free_memory_excluding_mig + runai_node_gpu_used_memory_excluding_mig
      record: runai_node_gpu_total_memory_excluding_mig
    - expr: sum by(node_name, node, gpu) (runai_mig_gpu_total_memory OR label_replace(runai_node_gpu_total_memory_excluding_mig,
        "node_name", "$0", "node", ".*"))
      record: runai_node_gpu_total_memory
    - expr: sum by(node_name, node, gpu) (runai_mig_gpu_used_memory OR label_replace(runai_node_gpu_used_memory_excluding_mig,
        "node_name", "$0", "node", ".*"))
      record: runai_node_gpu_used_memory
    - expr: timestamp(dcgm_gpu_utilization > 2) or runai_gpu_last_active_time_info_per_pod
        or DCGM_GPU_LAST_NOT_IDLE_TIME or timestamp(dcgm_gpu_utilization)
      record: runai_gpu_last_active_time_info_per_pod
    - expr: topk by(UUID, container, device, endpoint, gpu, instance, modelName, service)(1,
        runai_gpu_last_active_time_info_per_pod)
      record: runai_gpu_last_active_time_info
    - expr: sum without (device, endpoint, instance, job, namespace, pod, pod_ip,
        service)((label_replace(runai_gpu_last_active_time_info, "pod_ip", "$1", "instance",
        "(.*):(.*)")) * on (pod_ip) group_left(node) kube_pod_info{created_by_name=~".*dcgm-exporter"})
      record: runai_node_gpu_last_not_idle_time
    - expr: ((sum by (pod_group_uuid) (runai_pod_group_gpu_utilization_by_gpu)) /
        (count by (pod_group_uuid) (runai_pod_group_gpu_utilization_by_gpu))) * on
        (pod_group_uuid) group_left(queue_name, workload_name, project, job_name,
        job_uuid) (runai_pod_group_phase_with_info{phase="Running"}==1)
      record: runai_pod_group_gpu_utilization
    - expr: kube_pod_container_info{namespace="runai"}
      record: runai_pod_container_info
    - expr: kube_pod_status_phase{namespace="runai"}==1
      record: runai_pod_status_phase
    - expr: count(label_replace(sum(runai_pod_phase_with_info{gpu_index=~"^[0-9]+$",
        phase="Running"}==1) by(node_name, node, gpu_index, gpu), "node", "$1", "node_name",
        "(.+)")) by (node, gpu_index, gpu) or sum (runai_gpus_is_running_with_pod2)
        by (node) * 0
      record: runai_used_shared_gpu_per_node
    - expr: avg by (pod_group_uuid, gpu, node_name, node) (label_replace((runai_pod_phase_with_info{gpu_index=~"^[0-9]+$",
        phase="Running"} ==1), "gpu", "$1", "gpu_index", "(.+)") * on(node_name, node,
        gpu) group_left() label_replace(runai_node_gpu_utilization, "node_name", "$1",
        "node", "(.+)"))
      record: runai_utilization_shared_gpu_jobs
    - expr: sum by(pod_group_uuid, job_name, pod_group_uuid, job_uuid, gpu, node_name,
        node) ((runai_pod_phase_with_info{phase="Running"} ==1) * on (pod_name, pod_namespace)
        group_right(pod_uuid, pod_group_uuid, workload_name, node_name, node, job_name,
        job_uuid, project) dcgm_gpu_utilization)
      record: runai_utilization_full_gpu_jobs
    - expr: (runai_utilization_full_gpu_jobs or runai_utilization_shared_gpu_jobs)
      record: runai_pod_group_gpu_utilization_by_gpu
    - expr: label_replace(label_replace(sum by (exported_pod, exported_namespace)
        (DCGM_FI_DEV_FB_USED), "pod_name", "$1", "exported_pod", "(.*)"), "pod_namespace",
        "$1", "exported_namespace", "(.*)")
      record: runai_dedicated_gpu_pod_used_memory
    - expr: runai_pod_info * on(pod_name, pod_namespace) group_left() runai_dedicated_gpu_pod_used_memory
      record: runai_dedicated_gpu_pod_used_memory_with_pod_info
    - expr: sum by(pod_group_uuid) (runai_dedicated_gpu_pod_used_memory_with_pod_info)
      record: runai_dedicated_gpu_pod_group_used_memory_inner
    - expr: runai_pod_group_info * on (pod_group_uuid) group_left() (runai_dedicated_gpu_pod_group_used_memory_inner)
      record: runai_dedicated_gpu_pod_group_used_memory
    - expr: avg by(pod_group_uuid, job_uuid, gpu, node_name, node, job_name) (label_replace((runai_pod_phase_with_info{gpu_index=~"^[0-9]+$",phase="Running"}
        == 1), "gpu", "$1", "gpu_index", "(.+)") * on(node_name, node, gpu) group_left()
        label_replace(runai_node_gpu_used_memory, "node_name", "$1", "node", "(.+)"))
      record: runai_shared_gpu_pod_group_used_memory
    - expr: (runai_dedicated_gpu_pod_group_used_memory or runai_shared_gpu_pod_group_used_memory)
      record: runai_pod_group_used_gpu_memory_by_gpu
    - expr: sum by(pod_group_uuid, job_uuid) (runai_pod_group_used_gpu_memory_by_gpu)
      record: runai_pod_group_used_gpu_memory
    - expr: sum without (container, job, namespace, pod, service, endpoint, instance)
        (instance:node_cpu_utilisation:rate5m * on (pod, namespace) group_left(node)
        (kube_pod_info *0+1))
      record: runai_node_cpu_utilization
    - expr: avg(1-  (rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])))
      record: runai_cpu_utilization
    - expr: sum by (workload_name, pod_group_uuid, job_name, job_uuid , pod_namespace,
        project) (label_replace(label_replace(sum(rate(container_cpu_usage_seconds_total{
        container!=""}[1m])) by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"),
        "pod_namespace" , "$1", "namespace", "(.*)") * on(pod_name, pod_namespace)
        group_left(workload_name, pod_group_uuid, job_name, job_uuid, project) (runai_pod_phase_with_info{phase="Running"}
        ==1))
      record: runai_job_cpu_usage
    - expr: rate(container_cpu_usage_seconds_total{container=""}[1m]) * on (pod, namespace)
        group_left(gpu, gpu_index, workload_name, workload_type) label_replace(label_replace(runai_pod_phase_with_info{phase="Running"}
        == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace",
        "(.*)")
      record: runai_pod_cpu_usage
    - expr: sum without (container, device, endpoint, instance, job, namespace, pod,
        pod_ip, service) (instance:node_memory_utilisation:ratio * on (pod, namespace)
        group_left(node) (kube_pod_info *0+1))
      record: runai_node_memory_utilization
    - expr: 1 - sum (node_memory_MemAvailable_bytes{job="node-exporter"})  / sum(node_memory_MemTotal_bytes{job="node-exporter"})
      record: runai_memory_utilization
    - expr: sum without (container, endpoint,instance, job,namespace,pod,service)
        ((node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"}
        ) * on (pod, namespace) group_left(node) kube_pod_info)
      record: runai_node_memory_used_bytes
    - expr: sum(runai_node_memory_used_bytes)
      record: runai_memory_used_bytes
    - expr: sum(label_replace(label_replace(sum(container_memory_usage_bytes{ container!=""})
        by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1",
        "namespace", "(.*)") * on(pod_namespace, pod_name) group_left(workload_name,
        pod_group_uuid, job_name, job_uuid, project) (runai_pod_info * 0 + 1)) by
        (workload_name, pod_group_uuid, job_name, job_uuid, project)
      record: runai_job_memory_used_bytes
    - expr: max(max( label_replace(label_replace(kube_pod_container_info ,"pod_name"
        , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name,
        pod_namespace, image) * on(pod_name, pod_namespace) group_left(workload_name,
        pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (image,
        workload_name, pod_group_uuid , job_name, job_uuid)
      record: runai_pod_group_image
    - expr: container_memory_usage_bytes{container=""} * on (pod, namespace) group_left(gpu,
        gpu_index, workload_name, workload_type) label_replace(label_replace(runai_pod_phase_with_info{phase="Running"}
        == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace",
        "(.*)")
      record: runai_pod_memory_used_bytes
    - expr: sum(kube_pod_container_resource_requests{resource="memory",unit="byte",node!=""}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
        by (node)
      record: runai_node_requested_memory_bytes
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="memory",unit="byte"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1),
        "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)"))
        by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
        pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name,
        pod_group_uuid, job_name, job_uuid)
      record: runai_active_job_memory_allocated_bytes
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="memory",unit="byte"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),
        "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)"))
        by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
        pod_group_uuid, job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name,
        pod_group_uuid, job_name, job_uuid)
      record: runai_active_job_memory_requested_bytes
    - expr: sum(kube_pod_container_resource_requests{resource="cpu",unit="core",node!=""}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
        by (node)
      record: runai_node_cpu_requested
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="cpu",unit="core"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1),"pod_name"
        , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name,
        pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid,
        job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid,
        job_name, job_uuid)
      record: runai_active_job_cpu_allocated_cores
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="cpu",unit="core"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
        , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name,
        pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid,
        job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid,
        job_name, job_uuid)
      record: runai_active_job_cpu_requested_cores
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="cpu"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
        , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name,
        pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid,
        job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid,
        job_name, job_uuid)
      record: runai_active_job_cpu_limits
    - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="memory"}
        * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
        , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by (pod_name,
        pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name, pod_group_uuid,
        job_name, job_uuid) (runai_pod_info * 0 + 1) ) by (workload_name, pod_group_uuid,
        job_name, job_uuid)
      record: runai_active_job_memory_limits
    - expr: sum(runai_active_job_cpu_allocated_cores * on (pod_group_uuid) group_left
        (project) runai_pod_group_info) by (project)
      record: runai_queue_allocated_cpu
    - expr: sum(runai_active_job_memory_allocated_cores * on (pod_group_uuid) group_left
        (project) runai_pod_group_info) by (project)
      record: runai_queue_allocated_memory
    - expr: runai_pod_info * on(gpu, node) group_left() runai_gpu_utilization_with_pod_info{pod_name!=""}
      record: runai_gpu_sharing_pod_gpu_utilization_with_pod_info
    - expr: runai_pod_info * on(pod_name) group_left(gpu) runai_gpu_utilization_with_pod_info{pod_name!=""}
      record: runai_dedicated_gpu_pod_gpu_utilization_with_pod_info
    - expr: runai_gpu_sharing_pod_gpu_utilization_with_pod_info or runai_dedicated_gpu_pod_gpu_utilization_with_pod_info
      record: runai_pod_gpu_utilization_with_pod_info
    - expr: count by (gpu, node) (runai_pod_gpu_utilization_with_pod_info{pod_name!~'^runai-reservation.*'})
      record: runai_gpu_pod_count
    - expr: runai_pod_gpu_utilization_with_pod_info / on(gpu, node) group_left() runai_gpu_pod_count
      record: runai_pod_normalized_gpu_utilization_with_pod_info
    - alert: RunaiAgentPullRateLow
      annotations:
        summary: Runai Agent fails pulling configuration from the backend (instance
          {{ $labels.instance }})
      expr: ((rate(successful_backend_sync_count{type="pull"}[5m])) or (absent(successful_backend_sync_count{type="pull"})
        * 0)) < 0.09
      for: 10m
      labels:
        severity: critical
    - alert: RunaiAgentPushRateLow
      annotations:
        summary: Runai Agent fails pushing configuration from the backend (instance
          {{ $labels.instance }})
      expr: ((rate(successful_backend_sync_count{type="push"}[5m])) or (absent(successful_backend_sync_count{type="push"})
        * 0)) < 0.09
      for: 10m
      labels:
        severity: critical
    - alert: RunaiDeploymentInsufficientReplicas
      annotations:
        summary: Runai Deployment has not matched the expected number of replicas.
      expr: sum(min_over_time(kube_deployment_status_replicas_available{namespace=~"runai|runai-backend"}[30s]))
        < sum(min_over_time(kube_deployment_spec_replicas{namespace=~"runai|runai-backend"}[30s]))
      for: 5m
      labels:
        severity: critical
    - alert: RunaiProjectControllerReconcileFailure
      annotations:
        summary: Runai project controller service had a runtime error while reconciling
      expr: increase(controller_runtime_reconcile_errors_total{controller="project"}[10m])
        > 0
      labels:
        severity: critical
    - alert: RunaiStatefulSetInsufficientReplicas
      annotations:
        summary: Runai StatefulSet has not matched the expected number of replicas.
      expr: sum(min_over_time(kube_statefulset_status_replicas_ready{namespace=~"runai|runai-backend"}[30s]))
        < sum(min_over_time(kube_statefulset_status_replicas{namespace=~"runai|runai-backend"}[30s]))
      for: 5m
      labels:
        severity: critical
    - alert: RunaiCriticalProblem
      annotations:
        summary: Runai platform has a critical problem
      expr: sum(ALERTS{alertname=~"Runai.*", severity="critical"} unless ALERTS{alertname="RunaiCriticalProblem"})>
        0
      for: 5m
      labels:
        severity: critical
    - alert: RunaiDaemonSetRolloutStuck
      annotations:
        description: Runai DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} has not finished or progressed for at least 5 minutes.
        summary: Runai DaemonSet rollout is stuck.
      expr: |
        (
          (
            kube_daemonset_status_current_number_scheduled{namespace=~"runai|runai-backend"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
          ) or (
            kube_daemonset_status_number_misscheduled{namespace=~"runai|runai-backend"}
             !=
            0
          ) or (
            kube_daemonset_updated_number_scheduled{namespace=~"runai|runai-backend"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
          ) or (
            kube_daemonset_status_number_available{namespace=~"runai|runai-backend"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}
          )
        ) and (
          changes(kube_daemonset_updated_number_scheduled{namespace=~"runai|runai-backend"}[5m])
            ==
          0
        )
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_active_job_cpu_limits)
      record: runai_running_job_cpu_limit_cores
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_active_job_cpu_requested_cores)
      record: runai_running_job_cpu_requested_cores
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name)(runai_active_job_memory_limits)
      record: runai_running_job_memory_limit_bytes
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name)(runai_active_job_memory_requested_bytes)
      record: runai_running_job_memory_requested_bytes
    - expr: sum without(container, endpoint,instance, job,namespace,pod,service, pod_group_name,
        pod_group_uuid, queue_name, workload_name, workload_type)(runai_allocated_gpus)
      record: runai_job_allocated_gpus
    - expr: sum without(pod_group_name, pod_group_uuid) (runai_cpu_utilization)
      record: runai_cluster_cpu_utilization
    - expr: sum without(instance, pod_ip)(runai_gpus_is_running_with_pod2)
      record: runai_gpu_is_allocated
    - expr: sum without(pod_group_name, pod_group_uuid, pod_namespace, workload_name)(runai_job_cpu_usage)
      record: runai_running_job_cpu_used_cores
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_job_memory_used_bytes)
      record: runai_running_job_memory_used_bytes
    - expr: (runai_memory_used_bytes)
      record: runai_cluster_memory_used_bytes
    - expr: (runai_memory_utilization)
      record: runai_cluster_memory_utilization
    - expr: (runai_node_cpu_requested)
      record: runai_node_cpu_requested_cores
    - expr: sum without(UUID, Hostname, container, container_name, device, endpoint,
        exported_container, exported_namespace, exported_pod, instance, modelName,
        namespace, pod, pod_ip, pod_name, pod_namespace, service)(runai_node_gpu_last_not_idle_time)
      record: runai_gpu_last_active_time
    - expr: sum without(node_name)(runai_node_gpu_total_memory)
      record: runai_node_total_memory_bytes
    - expr: sum without(container, endpoint, instance, job, namespace, pod, service,
        exporter_container) (runai_node_memory_used_bytes)
      record: runai_node_used_memory_bytes
    - expr: sum without(pod_group_uuid, queue_name, workload_name)(runai_pod_group_gpu_utilization)
      record: runai_job_gpu_utilization
    - expr: sum without(pod_group_name, pod_group_uuid, workload_name) (runai_pod_group_image)
      record: runai_job_image
    - expr: sum without(pod_group_uuid, queue_name, workload_name, workload_type,
        job_uuid, node_name, phase)(runai_pod_group_phase_with_info)
      record: runai_job_status_with_info
    - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_uuid,service)(runai_pod_group_total_runtime)
      record: runai_job_total_runtime
    - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_uuid,service)(runai_pod_group_total_wait_time)
      record: runai_job_total_wait_time
    - expr: sum without(pod_group_uuid)(runai_pod_group_used_gpu_memory)
      record: runai_job_used_gpu_memory_bytes
    - expr: sum without(pod_group_uuid, node_name)(runai_pod_group_used_gpu_memory_by_gpu)
      record: runai_job_used_gpu_memory_bytes_with_gpu_node
    - expr: sum without(endpoint, instance, job,namespace, pod, queue_name,service)(runai_queue_deserved_gpus)
      record: runai_project_guaranteed_gpus
    - expr: sum without(endpoint, instance, job,namespace, pod, queue_name, service,
        deserved_cpu, deserved_gpu, deserved_memory)(runai_queue_info)
      record: runai_project_info
    - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_name, pod_group_uuid,queue_name,service,
        workload_name, workload_type)(runai_requested_gpus)
      record: runai_job_requested_gpus
    - expr: sum without(endpoint, instance, job,namespace, pod, pod_group_name, pod_group_uuid,queue_name,
        service, workload_name, workload_type)(runai_requested_gpus_memory)
      record: runai_job_requested_gpu_memory
    - expr: sum without(gpu_index)(runai_used_shared_gpu_per_node)
      record: runai_gpu_is_running_fractional_job
    - expr: sum without(pod_group_uuid, node_name)(runai_utilization_full_gpu_jobs)
      record: runai_gpu_utilization_non_fractional_jobs
    - expr: sum without(node_name)(runai_node_gpu_used_memory)
      record: runai_node_gpu_used_memory_bytes
    - expr: runai_node_gpu_utilization
      record: runai_gpu_utilization_with_pod_info
    - expr: kube_node_status_condition{condition="Ready", status="true"}
      record: runai_node_is_ready
    - expr: count((runai_node_is_ready * on (node) runai_node_gpu_count) > 0) or vector(0)
      record: runai_cluster_ready_gpu_node_count
    - expr: sum(runai_node_gpu_count)
      record: runai_cluster_gpu_count
    - expr: sum(runai_pod_info{gpu=""} and on(pod_uuid) runai_pod_phase{phase="Running"}==1)
        or vector(0)
      record: runai_cluster_allocated_dedicated_gpu_count
    - expr: count(group(runai_pod_info{gpu!=""} and on(pod_uuid) runai_pod_phase{phase="Running"}==1)
        by (gpu, node)) or vector(0)
      record: runai_cluster_allocated_shared_gpu_count
    - expr: (runai_cluster_allocated_shared_gpu_count + runai_cluster_allocated_dedicated_gpu_count)
        or vector(0)
      record: runai_cluster_allocated_gpu_count
    - expr: sum(runai_pod_provisioned_gpu_count * on(pod_uuid) group_left(project)
        runai_pod_info) by(project) or (group(runai_project_info) by (project) * 0)
      record: runai_project_provisioned_gpu_count
    - expr: sum(runai_project_provisioned_gpu_count) or vector(0)
      record: runai_cluster_provisioned_gpu_count
    - expr: group(kube_pod_info * on(pod) group_right(node) (clamp_max(max_over_time(dcgm_gpu_utilization[5m]),
        1))) by (node, gpu)
      record: runai_gpu_is_idle
    - expr: sum without (revision_name) (label_replace((sum by(clusterId, revision_name,
        namespace_name) (rate(revision_app_request_count[1m]))),"deployment_name",
        "$1-deployment", "revision_name", "(.+)"))
      record: runai_deployment_request_rate
    - expr: sum without (revision_name) (label_replace( (sum by (clusterId, namespace_name,
        revision_name, le) (rate (revision_app_request_latencies_bucket[60s]))) ,
        "deployment_name", "$1-deployment", "revision_name", "(.+)"))
      record: runai_deployment_request_latencies
  - interval: 1s
    name: runai-rules-short-interval
    rules:
    - expr: sum_over_time(avg(runai_pod_group_info{detailed_status="Running"}) by
        (workload_name, job_name, endpoint, instance, job, job_uuid, namespace, pod,
        pod_group_uuid, service)[1s:100ms]) / 10
      record: runai_pod_group_run_time_per_second
    - expr: runai_pod_group_run_time_per_second + (runai_pod_group_total_runtime or
        runai_pod_group_run_time_per_second * 0)
      record: runai_pod_group_total_runtime
    - expr: sum_over_time(avg(runai_pod_group_info{detailed_status=~"Pending|Swapped"})
        by (workload_name, job_name, endpoint, instance, job, job_uuid, namespace,
        pod, pod_group_uuid, service)[1s:100ms]) / 10
      record: runai_pod_group_wait_time_per_second
    - expr: runai_pod_group_wait_time_per_second + (runai_pod_group_total_wait_time
        or runai_pod_group_wait_time_per_second * 0)
      record: runai_pod_group_total_wait_time
    - expr: sum_over_time(avg(runai_workload_info{status="Running"}) by (uuid)[1s:100ms])
        / 10
      record: runai_workload_run_time_per_second
    - expr: runai_workload_run_time_per_second + (runai_workload_total_run_time or
        runai_workload_run_time_per_second * 0)
      record: runai_workload_total_run_time
    - expr: sum_over_time(avg(runai_workload_info{status=~"Pending|Swapped"}) by (uuid)[1s:100ms])
        / 10
      record: runai_workload_wait_time_per_second
    - expr: runai_workload_wait_time_per_second + (runai_workload_total_wait_time
        or runai_workload_wait_time_per_second * 0)
      record: runai_workload_total_wait_time
